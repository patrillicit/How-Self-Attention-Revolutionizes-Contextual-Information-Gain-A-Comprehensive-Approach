# How-Self-Attention-Revolutionizes-Contextual-Information-Gain-A-Comprehensive-Approach
This work elaborates on the architecture and advantages of the self-attention mechanism in the Encoder of Transformer Neural Networks. Self-attention is a tool used to gain more contextualized information from raw input data like texts or images efficiently. It puts attention on relevant information in the data to rapidly extract the important contents comparable to humans, pulling specifically needed information from all the endless input data detected by their senses.

Review paper on Multi-Headed-Self-Attention in Transformer Neural Networks.

Patrick Gottschling (1155173254@link.cuhk.edu.hk)
Department of Computer Science and Engineering
The Chinese University of Hong Kong
(January 2022)
