# How-Self-Attention-Revolutionizes-Contextual-Information-Gain-A-Comprehensive-Approach
This work elaborates on the architecture and advantages of the self-attention mechanism in the Encoder of Transformer Neural Networks. Self-attention is a tool used to gain more contextualized information from raw input data like texts or images efficiently. It puts attention on relevant information in the data to rapidly extract the important contents comparable to humans, pulling specifically needed information from all the endless input data detected by their senses.
